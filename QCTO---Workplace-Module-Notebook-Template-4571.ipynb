{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCTO - Workplace Module\n",
    "\n",
    "### Project Title: Please Insert your Project Title Here\n",
<<<<<<< HEAD
    "#### Done By: Name and Surname\n",
=======
    "#### Done By: Lindiwe Masuku\n",
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
    "\n",
    "¬© ExploreAI 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#BC> Background Context</a>\n",
    "\n",
    "<a href=#one>1. Importing Packages</a>\n",
    "\n",
    "<a href=#two>2. Data Collection and Description</a>\n",
    "\n",
    "<a href=#three>3. Loading Data </a>\n",
    "\n",
    "<a href=#four>4. Data Cleaning and Filtering</a>\n",
    "\n",
    "<a href=#five>5. Exploratory Data Analysis (EDA)</a>\n",
    "\n",
    "<a href=#six>6. Modeling </a>\n",
    "\n",
    "<a href=#seven>7. Evaluation and Validation</a>\n",
    "\n",
    "<a href=#eight>8. Final Model</a>\n",
    "\n",
    "<a href=#nine>9. Conclusion and Future Work</a>\n",
    "\n",
    "<a href=#ten>10. References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    " <a id=\"BC\"></a>\n",
    "## **Background Context**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Introduce the project, outline its goals, and explain its significance.\n",
    "* **Details:** Include information about the problem domain, the specific questions or challenges the project aims to address, and any relevant background information that sets the stage for the work.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#one></a>\n",
    "## **Importing Packages**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Set up the Python environment with necessary libraries and tools.\n",
    "* **Details:** List and import all the Python packages that will be used throughout the project such as Pandas for data manipulation, Matplotlib/Seaborn for visualization, scikit-learn for modeling, etc.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#Please use code cells to code in and do not forget to comment your code."
=======
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def load_india_data():\n",
    "    '''Load the India dataset'''\n",
    "    print('=== LOADING INDIA DATASET ===')\n",
    "    \n",
    "    # Try to load from current directory or data folder\n",
    "    data_paths = ['india_data.csv', 'Data/india_data.csv', '../india-dataset/data.csv']\n",
    "    \n",
    "    for path in data_paths:\n",
    "        if os.path.exists(path):\n",
    "            india_df = pd.read_csv(path)\n",
    "            print(f'Loaded India data from {path}: {india_df.shape}')\n",
    "            return india_df\n",
    "    \n",
    "    print('Could not find India dataset')\n",
    "    return None\n",
    "\n",
    "def explore_india_data():\n",
    "    '''Explore and analyze the India dataset'''\n",
    "    \n",
    "    # Load data\n",
    "    df = load_india_data()\n",
    "    \n",
    "    if df is None:\n",
    "        print('Cannot proceed without data files')\n",
    "        return\n",
    "    \n",
    "    print('\\n=== DATA EXPLORATION ===')\n",
    "    print(f'Dataset shape: {df.shape}')\n",
    "    print('Columns:', df.columns.tolist())\n",
    "    print('\\nFirst few rows:')\n",
    "    print(df.head())\n",
    "    \n",
    "    print('\\nData types:')\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print('\\nMissing values:')\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "    print('\\nBasic statistics:')\n",
    "    print(df.describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    explore_india_data()"
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#two></a>\n",
    "## **Data Collection and Description**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Describe how the data was collected and provide an overview of its characteristics.\n",
    "* **Details:** Mention sources of the data, the methods used for collection (e.g., APIs, web scraping, datasets from repositories), and a general description of the dataset including size, scope, and types of data available (e.g., numerical, categorical).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#Please use code cells to code in and do not forget to comment your code."
=======
    "# Data Collection and Description for India Dataset\n",
    "# This section describes the data source and collection methodology\n",
    "\n",
    "print(\"=== INDIA DATASET - DATA COLLECTION AND DESCRIPTION ===\\n\")\n",
    "\n",
    "# Data Source Information\n",
    "print(\"üìä DATA SOURCE:\")\n",
    "print(\"- Repository: https://github.com/lindiwemasuku89/Capstone-Project-Report\")\n",
    "print(\"- Dataset: India Dataset\")\n",
    "print(\"- Collection Method: GitHub Repository Download\")\n",
    "print(\"- Data Format: CSV (Comma Separated Values)\\n\")\n",
    "\n",
    "# Attempt to load and describe the dataset\n",
    "try:\n",
    "    # Load the India dataset\n",
    "    df = load_india_data()\n",
    "    \n",
    "    if df is not None:\n",
    "        print(\"‚úÖ DATASET SUCCESSFULLY LOADED\\n\")\n",
    "        \n",
    "        # Basic Dataset Information\n",
    "        print(\"üìà DATASET CHARACTERISTICS:\")\n",
    "        print(f\"- Total Records: {df.shape[0]:,}\")\n",
    "        print(f\"- Total Features: {df.shape[1]}\")\n",
    "        print(f\"- Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Column Information\n",
    "        print(f\"\\nüìã AVAILABLE COLUMNS ({len(df.columns)}):\")\n",
    "        for i, col in enumerate(df.columns, 1):\n",
    "            dtype = str(df[col].dtype)\n",
    "            non_null = df[col].count()\n",
    "            null_count = df[col].isnull().sum()\n",
    "            print(f\"  {i:2d}. {col:<20} | Type: {dtype:<10} | Non-null: {non_null:,} | Missing: {null_count:,}\")\n",
    "        \n",
    "        # Data Types Summary\n",
    "        print(f\"\\nüî¢ DATA TYPES SUMMARY:\")\n",
    "        dtype_counts = df.dtypes.value_counts()\n",
    "        for dtype, count in dtype_counts.items():\n",
    "            print(f\"- {dtype}: {count} columns\")\n",
    "        \n",
    "        # Missing Data Overview\n",
    "        total_missing = df.isnull().sum().sum()\n",
    "        missing_percentage = (total_missing / (df.shape[0] * df.shape[1])) * 100\n",
    "        print(f\"\\n‚ùå MISSING DATA OVERVIEW:\")\n",
    "        print(f\"- Total Missing Values: {total_missing:,}\")\n",
    "        print(f\"- Missing Data Percentage: {missing_percentage:.2f}%\")\n",
    "        \n",
    "        if total_missing > 0:\n",
    "            print(\"\\nColumns with Missing Values:\")\n",
    "            missing_data = df.isnull().sum()[df.isnull().sum() > 0].sort_values(ascending=False)\n",
    "            for col, missing in missing_data.items():\n",
    "                percentage = (missing / len(df)) * 100\n",
    "                print(f\"  - {col}: {missing:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå DATASET NOT FOUND\")\n",
    "        print(\"Please ensure the India dataset is available in one of these locations:\")\n",
    "        print(\"- ./india_data.csv\")\n",
    "        print(\"- ./Data/india_data.csv\")\n",
    "        print(\"- ../india-dataset/data.csv\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR LOADING DATASET: {str(e)}\")\n",
    "    print(\"\\nüìù EXPECTED DATASET DESCRIPTION:\")\n",
    "    print(\"- Source: India Dataset from GitHub Repository\")\n",
    "    print(\"- Expected Format: CSV file\")\n",
    "    print(\"- Content: Various indicators related to India\")\n",
    "    print(\"- Typical Features: Demographic, economic, or social indicators\")\n",
    "    print(\"- Data Period: [To be determined upon data loading]\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Next Step: Load the actual dataset to proceed with analysis\")\n",
    "print(\"=\"*60)"
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#three></a>\n",
    "## **Loading Data**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Load the data into the notebook for manipulation and analysis.\n",
    "* **Details:** Show the code used to load the data and display the first few rows to give a sense of what the raw data looks like.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#Please use code cells to code in and do not forget to comment your code."
=======
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "# Loading Data Section - India Dataset\n",
    "# This section loads the India dataset and displays initial data exploration\n",
    "\n",
    "print(\"=== LOADING DATA SECTION ===\\n\")\n",
    "\n",
    "# Load the India dataset using our predefined function\n",
    "print(\"üìÇ LOADING INDIA DATASET...\")\n",
    "df = load_india_data()\n",
    "\n",
    "# Check if data was loaded successfully\n",
    "if df is not None:\n",
    "    print(\"‚úÖ DATA LOADED SUCCESSFULLY!\\n\")\n",
    "    \n",
    "    # Display basic dataset information\n",
    "    print(\"üìä DATASET OVERVIEW:\")\n",
    "    print(f\"- Shape: {df.shape[0]} rows √ó {df.shape[1]} columns\")\n",
    "    print(f\"- Size: {df.size:,} total data points\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\nüìã FIRST 5 ROWS OF THE DATASET:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.head())\n",
    "    \n",
    "    # Display last few rows\n",
    "    print(f\"\\nüìã LAST 5 ROWS OF THE DATASET:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df.tail())\n",
    "    \n",
    "    # Display column names and their data types\n",
    "    print(f\"\\nüè∑Ô∏è COLUMN INFORMATION:\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, (col, dtype) in enumerate(zip(df.columns, df.dtypes), 1):\n",
    "        sample_value = df[col].dropna().iloc[0] if not df[col].dropna().empty else \"N/A\"\n",
    "        print(f\"{i:2d}. {col:<25} | Type: {str(dtype):<12} | Sample: {sample_value}\")\n",
    "    \n",
    "    # Display basic statistics\n",
    "    print(f\"\\nüìà BASIC STATISTICS:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Numerical columns statistics\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        print(\"NUMERICAL COLUMNS:\")\n",
    "        print(df[numerical_cols].describe())\n",
    "    \n",
    "    # Categorical columns information\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nüìù CATEGORICAL COLUMNS SUMMARY:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"- {col}: {unique_count} unique values\")\n",
    "            if unique_count <= 10:  # Show values if not too many\n",
    "                print(f\"  Values: {list(df[col].unique())}\")\n",
    "    \n",
    "    # Memory usage information\n",
    "    print(f\"\\nüíæ MEMORY USAGE:\")\n",
    "    memory_usage = df.memory_usage(deep=True)\n",
    "    total_memory = memory_usage.sum() / (1024 ** 2)  # Convert to MB\n",
    "    print(f\"- Total Memory Usage: {total_memory:.2f} MB\")\n",
    "    print(f\"- Average per column: {total_memory/len(df.columns):.2f} MB\")\n",
    "    \n",
    "    # Check for duplicate rows\n",
    "    duplicate_count = df.duplicated().sum()\n",
    "    print(f\"\\nüîç DATA QUALITY CHECKS:\")\n",
    "    print(f\"- Duplicate rows: {duplicate_count}\")\n",
    "    print(f\"- Unique rows: {len(df) - duplicate_count}\")\n",
    "    \n",
    "    # Store the loaded dataframe for use in subsequent sections\n",
    "    globals()['india_df'] = df\n",
    "    print(f\"\\n‚úÖ Dataset stored in variable 'india_df' for further analysis\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå FAILED TO LOAD DATASET\")\n",
    "    print(\"\\nüîß TROUBLESHOOTING STEPS:\")\n",
    "    print(\"1. Ensure the India dataset CSV file is in your working directory\")\n",
    "    print(\"2. Check that the file name matches one of these patterns:\")\n",
    "    print(\"   - india_data.csv\")\n",
    "    print(\"   - Data/india_data.csv\") \n",
    "    print(\"   - ../india-dataset/data.csv\")\n",
    "    print(\"3. Verify the file is not corrupted and is in proper CSV format\")\n",
    "    print(\"4. Check file permissions\")\n",
    "    \n",
    "    # Provide alternative loading methods\n",
    "    print(f\"\\nüìÅ ALTERNATIVE LOADING METHODS:\")\n",
    "    print(\"# If your file has a different name or location, use:\")\n",
    "    print(\"# df = pd.read_csv('your_file_path_here.csv')\")\n",
    "    print(\"# For example:\")\n",
    "    print(\"# df = pd.read_csv('C:/Users/YourName/Documents/india_dataset.csv')\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA LOADING SECTION COMPLETE\")\n",
    "print(\"=\"*80)"
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#four></a>\n",
    "## **Data Cleaning and Filtering**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Prepare the data for analysis by cleaning and filtering.\n",
    "* **Details:** Include steps for handling missing values, removing outliers, correcting errors, and possibly reducing the data (filtering based on certain criteria or features).\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#Please use code cells to code in and do not forget to comment your code."
=======
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "# ...existing code...\n",
    "# Data Cleaning and Filtering - Indian Agriculture Dataset\n",
    "print(\"=== DATA CLEANING AND FILTERING SECTION ===\\n\")\n",
    "\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    if 'india_df' in globals():\n",
    "        df = india_df.copy()\n",
    "        print(\"‚úÖ Using 'india_df' from previous cell\")\n",
    "    else:\n",
    "        df = load_india_data()\n",
    "        if df is None:\n",
    "            raise RuntimeError(\"Dataset not available\")\n",
    "\n",
    "    print(f\"üìä Starting with shape: {df.shape}\\n\")\n",
    "\n",
    "    # Standardize column names\n",
    "    col_map = {\n",
    "        'State': 'State_Name', 'state': 'State_Name',\n",
    "        'District': 'District_Name', 'district': 'District_Name',\n",
    "        'Year': 'Crop_Year', 'year': 'Crop_Year',\n",
    "        'Area_Hectares': 'Area', 'Area_hectares': 'Area',\n",
    "        'Production_tonnes': 'Production'\n",
    "    }\n",
    "    df.rename(columns={c:col_map[c] for c in df.columns if c in col_map}, inplace=True)\n",
    "    print(\"üìã Columns after rename:\", list(df.columns))\n",
    "\n",
    "    # Basic info\n",
    "    print(\"\\n--- Missing values by column ---\")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    # Clean text columns: strip + title case\n",
    "    for cat in ['State_Name','District_Name','Crop','Season']:\n",
    "        if cat in df.columns:\n",
    "            df[cat] = df[cat].astype(str).str.strip().replace({'nan': None})\n",
    "            df.loc[df[cat].notnull(), cat] = df.loc[df[cat].notnull(), cat].str.title()\n",
    "\n",
    "    # Handle missing values (contextual)\n",
    "    # Drop rows with missing location if few; otherwise mark Unknown\n",
    "    if 'State_Name' in df.columns:\n",
    "        miss = df['State_Name'].isnull().sum()\n",
    "        if miss > 0:\n",
    "            pct = miss / len(df) * 100\n",
    "            if pct < 10:\n",
    "                df.dropna(subset=['State_Name'], inplace=True)\n",
    "            else:\n",
    "                df['State_Name'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Fill categorical modes\n",
    "    for col in ['District_Name','Crop','Season']:\n",
    "        if col in df.columns:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                mode = df[col].mode()\n",
    "                if not mode.empty:\n",
    "                    df[col].fillna(mode.iloc[0], inplace=True)\n",
    "                else:\n",
    "                    df[col].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Numeric columns: median fill if missing is small, else drop\n",
    "    for col in ['Area','Production','Crop_Year']:\n",
    "        if col in df.columns:\n",
    "            miss = df[col].isnull().sum()\n",
    "            if miss > 0:\n",
    "                pct = miss / len(df) * 100\n",
    "                if pct < 20:\n",
    "                    df[col].fillna(df[col].median(), inplace=True)\n",
    "                else:\n",
    "                    df.dropna(subset=[col], inplace=True)\n",
    "\n",
    "    # Convert numeric dtypes\n",
    "    for ncol in ['Area','Production','Crop_Year']:\n",
    "        if ncol in df.columns:\n",
    "            df[ncol] = pd.to_numeric(df[ncol], errors='coerce')\n",
    "\n",
    "    # Remove exact duplicates\n",
    "    dup = df.duplicated().sum()\n",
    "    if dup > 0:\n",
    "        df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "        print(f\"‚úÖ Removed {dup} exact duplicate rows\")\n",
    "\n",
    "    # Logical duplicates check (State, District, Crop, Year, Season)\n",
    "    keys = [k for k in ['State_Name','District_Name','Crop','Crop_Year','Season'] if k in df.columns]\n",
    "    if len(keys) >= 3:\n",
    "        logical_dup = df.duplicated(subset=keys).sum()\n",
    "        if logical_dup > 0:\n",
    "            print(f\"‚ö†Ô∏è Logical duplicates detected: {logical_dup} (kept for review)\")\n",
    "\n",
    "    # Validate numeric ranges\n",
    "    if 'Area' in df.columns:\n",
    "        neg_area = (df['Area'] < 0).sum()\n",
    "        if neg_area > 0:\n",
    "            df['Area'] = df['Area'].abs()\n",
    "        unrealistic_area = (df['Area'] > 1_000_000).sum()\n",
    "        if unrealistic_area > 0:\n",
    "            print(f\"‚ö†Ô∏è {unrealistic_area} records have very large Area values\")\n",
    "\n",
    "    if 'Production' in df.columns:\n",
    "        neg_prod = (df['Production'] < 0).sum()\n",
    "        if neg_prod > 0:\n",
    "            df['Production'] = df['Production'].abs()\n",
    "\n",
    "    # Productivity\n",
    "    if set(['Area','Production']).issubset(df.columns):\n",
    "        df['Productivity'] = df['Production'] / (df['Area'] + 1e-6)\n",
    "        high_prod = (df['Productivity'] > 100).sum()\n",
    "        if high_prod > 0:\n",
    "            print(f\"‚ö†Ô∏è {high_prod} records have Productivity > 100 t/ha (review)\")\n",
    "\n",
    "    # Downcast numerics for memory\n",
    "    for col in df.select_dtypes(include=['int64','float64']).columns:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "\n",
    "    # Final summary\n",
    "    print(\"\\n--- Cleaning summary ---\")\n",
    "    print(f\"Final shape: {df.shape}\")\n",
    "    print(\"Remaining missing values:\", df.isnull().sum().sum())\n",
    "\n",
    "    globals()['cleaned_df'] = df\n",
    "    print(\"‚úÖ Cleaned data stored as 'cleaned_df'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error during cleaning:\", e)\n",
    "    import traceback; traceback.print_exc()\n",
    "# ...existing code..."
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#five></a>\n",
    "## **Exploratory Data Analysis (EDA)**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Explore and visualize the data to uncover patterns, trends, and relationships.\n",
    "* **Details:** Use statistics and visualizations to explore the data. This may include histograms, box plots, scatter plots, and correlation matrices. Discuss any significant findings.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#Please use code cells to code in and do not forget to comment your code."
=======
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "# ...existing code...\n",
    "# Exploratory Data Analysis (EDA)\n",
    "print(\"=== EXPLORATORY DATA ANALYSIS (EDA) ===\\n\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "try:\n",
    "    if 'cleaned_df' in globals():\n",
    "        df = cleaned_df.copy()\n",
    "        print(\"‚úÖ Using 'cleaned_df'\")\n",
    "    elif 'india_df' in globals():\n",
    "        df = india_df.copy()\n",
    "        print(\"‚úÖ Using 'india_df' (not cleaned)\")\n",
    "    else:\n",
    "        df = load_india_data()\n",
    "        print(\"‚ÑπÔ∏è Using sample dataset\")\n",
    "\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "    # Basic stats\n",
    "    if len(numeric_cols) > 0:\n",
    "        display(df[numeric_cols].describe().T)\n",
    "\n",
    "    # Top categories\n",
    "    for c in cat_cols:\n",
    "        print(f\"\\n-- {c} value counts (top 5) --\")\n",
    "        print(df[c].value_counts().head(5))\n",
    "\n",
    "    # Distributions (Area, Production, Productivity)\n",
    "    for col in ['Area','Production','Productivity']:\n",
    "        if col in df.columns:\n",
    "            plt.figure(figsize=(8,4))\n",
    "            sns.histplot(df[col].dropna(), kde=True)\n",
    "            plt.title(f\"Distribution: {col}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            plt.figure(figsize=(6,3))\n",
    "            sns.boxplot(x=df[col].dropna())\n",
    "            plt.title(f\"Boxplot: {col}\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    # Correlation heatmap (numerics)\n",
    "    if len(numeric_cols) > 1:\n",
    "        corr = df[numeric_cols].corr()\n",
    "        plt.figure(figsize=(8,6))\n",
    "        mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "        sns.heatmap(corr, annot=True, mask=mask, cmap='coolwarm', fmt='.2f')\n",
    "        plt.title(\"Correlation matrix\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Top crops by total production\n",
    "    if 'Crop' in df.columns and 'Production' in df.columns:\n",
    "        prod_by_crop = df.groupby('Crop', dropna=True)['Production'].sum().sort_values(ascending=False)\n",
    "        print(\"\\nTop crops by total production:\")\n",
    "        display(prod_by_crop.head(10))\n",
    "\n",
    "        plt.figure(figsize=(10,4))\n",
    "        prod_by_crop.head(10).plot(kind='bar')\n",
    "        plt.ylabel('Total Production')\n",
    "        plt.title('Top 10 Crops by Production')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Time trends for a sample crop (if years present)\n",
    "    if 'Crop_Year' in df.columns and 'Production' in df.columns:\n",
    "        sample = df.groupby('Crop_Year', dropna=True)['Production'].sum().sort_index()\n",
    "        plt.figure(figsize=(10,4))\n",
    "        sns.lineplot(x=sample.index, y=sample.values, marker='o')\n",
    "        plt.title('Total Production by Year')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Production')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Save EDA summary struct\n",
    "    eda_results = {\n",
    "        'numeric_columns': numeric_cols,\n",
    "        'categorical_columns': cat_cols\n",
    "    }\n",
    "    globals()['eda_results'] = eda_results\n",
    "    print(\"‚úÖ EDA complete. Results in 'eda_results'\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå EDA error:\", e)\n",
    "    import traceback; traceback.print_exc()\n",
    "# ...existing code...\n",
    "   "
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#six></a>\n",
    "## **Modeling**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Develop and train predictive or statistical models.\n",
    "* **Details:** Describe the choice of models, feature selection and engineering processes, and show how the models are trained. Include code for setting up the models and explanations of the model parameters.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#Please use code cells to code in and do not forget to comment your code."
=======
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "# Machine Learning Modeling - Indian Agriculture Dataset\n",
    "# This section develops and trains predictive models for agricultural data\n",
    "\n",
    "print(\"=== MACHINE LEARNING MODELING SECTION ===\\n\")\n",
    "\n",
    "# Import additional modeling libraries\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.svm import SVR, SVC\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    # Use cleaned data from previous sections\n",
    "    if 'cleaned_df' in globals():\n",
    "        df = cleaned_df.copy()\n",
    "        print(\"‚úÖ Using cleaned dataset\")\n",
    "    elif 'india_df' in globals():\n",
    "        df = india_df.copy()\n",
    "        print(\"‚úÖ Using original dataset\")\n",
    "    else:\n",
    "        df = load_india_data()\n",
    "        print(\"‚ö†Ô∏è  Using sample dataset\")\n",
    "    \n",
    "    print(f\"üìä Modeling dataset shape: {df.shape}\")\n",
    "    print(f\"üìã Available columns: {list(df.columns)}\\n\")\n",
    "    \n",
    "    # ===== STEP 1: DATA PREPARATION FOR MODELING =====\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 1: DATA PREPARATION FOR MODELING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Check available features for modeling\n",
    "    numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(f\"üìä Numerical features: {numerical_features}\")\n",
    "    print(f\"üìù Categorical features: {categorical_features}\")\n",
    "    \n",
    "    # Prepare modeling dataframe\n",
    "    modeling_df = df.copy()\n",
    "    \n",
    "    # Handle categorical variables with label encoding\n",
    "    label_encoders = {}\n",
    "    for col in categorical_features:\n",
    "        if col in modeling_df.columns:\n",
    "            le = LabelEncoder()\n",
    "            # Fill any remaining missing values\n",
    "            modeling_df[col] = modeling_df[col].fillna('Unknown')\n",
    "            modeling_df[col + '_encoded'] = le.fit_transform(modeling_df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "            print(f\"   ‚úÖ Encoded {col}: {modeling_df[col].nunique()} unique values\")\n",
    "    \n",
    "    # ===== STEP 2: DEFINE MODELING PROBLEMS =====\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 2: DEFINE MODELING PROBLEMS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    modeling_results = {}\n",
    "    \n",
    "    # PROBLEM 1: REGRESSION - Predict Production based on Area and other factors\n",
    "    if 'Production' in modeling_df.columns and 'Area' in modeling_df.columns:\n",
    "        print(\"\\nüéØ REGRESSION PROBLEM: Predicting Crop Production\")\n",
    "        \n",
    "        # Select features for production prediction\n",
    "        feature_cols = ['Area']\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        for col in categorical_features:\n",
    "            encoded_col = col + '_encoded'\n",
    "            if encoded_col in modeling_df.columns:\n",
    "                feature_cols.append(encoded_col)\n",
    "        \n",
    "        # Add other numerical features\n",
    "        for col in ['Crop_Year', 'Productivity']:\n",
    "            if col in modeling_df.columns and col != 'Production':\n",
    "                feature_cols.append(col)\n",
    "        \n",
    "        # Remove any missing values for this analysis\n",
    "        regression_df = modeling_df[feature_cols + ['Production']].dropna()\n",
    "        \n",
    "        if len(regression_df) > 10:  # Ensure we have enough data\n",
    "            X_reg = regression_df[feature_cols]\n",
    "            y_reg = regression_df['Production']\n",
    "            \n",
    "            print(f\"   üìä Regression features: {feature_cols}\")\n",
    "            print(f\"   üìä Regression dataset shape: {X_reg.shape}\")\n",
    "            print(f\"   üéØ Target variable: Production (range: {y_reg.min():.2f} - {y_reg.max():.2f})\")\n",
    "            \n",
    "            # Split the data\n",
    "            X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(\n",
    "                X_reg, y_reg, test_size=0.2, random_state=42\n",
    "            )\n",
    "            \n",
    "            # Scale the features\n",
    "            scaler_reg = StandardScaler()\n",
    "            X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "            X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "            \n",
    "            print(f\"   üìä Train set: {X_train_reg.shape}, Test set: {X_test_reg.shape}\")\n",
    "            \n",
    "            modeling_results['regression'] = {\n",
    "                'X_train': X_train_reg_scaled,\n",
    "                'X_test': X_test_reg_scaled,\n",
    "                'y_train': y_train_reg,\n",
    "                'y_test': y_test_reg,\n",
    "                'feature_names': feature_cols,\n",
    "                'scaler': scaler_reg\n",
    "            }\n",
    "        else:\n",
    "            print(\"   ‚ùå Insufficient data for regression modeling\")\n",
    "    \n",
    "    # PROBLEM 2: CLASSIFICATION - Predict High/Low Productivity\n",
    "    if 'Productivity' in modeling_df.columns:\n",
    "        print(f\"\\nüéØ CLASSIFICATION PROBLEM: Predicting Productivity Category\")\n",
    "        \n",
    "        # Create productivity categories\n",
    "        productivity_median = modeling_df['Productivity'].median()\n",
    "        modeling_df['Productivity_Category'] = (modeling_df['Productivity'] > productivity_median).astype(int)\n",
    "        \n",
    "        # Select features for classification\n",
    "        feature_cols_class = []\n",
    "        \n",
    "        # Add numerical features (excluding target-related)\n",
    "        for col in ['Area', 'Crop_Year']:\n",
    "            if col in modeling_df.columns:\n",
    "                feature_cols_class.append(col)\n",
    "        \n",
    "        # Add encoded categorical features\n",
    "        for col in categorical_features:\n",
    "            encoded_col = col + '_encoded'\n",
    "            if encoded_col in modeling_df.columns:\n",
    "                feature_cols_class.append(encoded_col)\n",
    "        \n",
    "        # Remove any missing values\n",
    "        classification_df = modeling_df[feature_cols_class + ['Productivity_Category']].dropna()\n",
    "        \n",
    "        if len(classification_df) > 10:\n",
    "            X_class = classification_df[feature_cols_class]\n",
    "            y_class = classification_df['Productivity_Category']\n",
    "            \n",
    "            print(f\"   üìä Classification features: {feature_cols_class}\")\n",
    "            print(f\"   üìä Classification dataset shape: {X_class.shape}\")\n",
    "            print(f\"   üéØ Target: Productivity Category (High=1: {(y_class==1).sum()}, Low=0: {(y_class==0).sum()})\")\n",
    "            \n",
    "            # Split the data\n",
    "            X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(\n",
    "                X_class, y_class, test_size=0.2, random_state=42, stratify=y_class\n",
    "            )\n",
    "            \n",
    "            # Scale the features\n",
    "            scaler_class = StandardScaler()\n",
    "            X_train_class_scaled = scaler_class.fit_transform(X_train_class)\n",
    "            X_test_class_scaled = scaler_class.transform(X_test_class)\n",
    "            \n",
    "            modeling_results['classification'] = {\n",
    "                'X_train': X_train_class_scaled,\n",
    "                'X_test': X_test_class_scaled,\n",
    "                'y_train': y_train_class,\n",
    "                'y_test': y_test_class,\n",
    "                'feature_names': feature_cols_class,\n",
    "                'scaler': scaler_class\n",
    "            }\n",
    "        else:\n",
    "            print(\"   ‚ùå Insufficient data for classification modeling\")\n",
    "    \n",
    "    # ===== STEP 3: REGRESSION MODELS =====\n",
    "    if 'regression' in modeling_results:\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 3: REGRESSION MODELS - PREDICTING PRODUCTION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        reg_data = modeling_results['regression']\n",
    "        \n",
    "        # Initialize regression models\n",
    "        regression_models = {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Ridge Regression': Ridge(alpha=1.0),\n",
    "            'Lasso Regression': Lasso(alpha=1.0),\n",
    "            'Decision Tree': DecisionTreeRegressor(random_state=42, max_depth=10),\n",
    "            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\n",
    "            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=6)\n",
    "        }\n",
    "        \n",
    "        regression_scores = {}\n",
    "        \n",
    "        print(\"üîß Training regression models...\")\n",
    "        \n",
    "        for name, model in regression_models.items():\n",
    "            try:\n",
    "                # Train the model\n",
    "                model.fit(reg_data['X_train'], reg_data['y_train'])\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred_train = model.predict(reg_data['X_train'])\n",
    "                y_pred_test = model.predict(reg_data['X_test'])\n",
    "                \n",
    "                # Calculate metrics\n",
    "                train_r2 = r2_score(reg_data['y_train'], y_pred_train)\n",
    "                test_r2 = r2_score(reg_data['y_test'], y_pred_test)\n",
    "                test_mse = mean_squared_error(reg_data['y_test'], y_pred_test)\n",
    "                test_mae = mean_absolute_error(reg_data['y_test'], y_pred_test)\n",
    "                test_rmse = np.sqrt(test_mse)\n",
    "                \n",
    "                regression_scores[name] = {\n",
    "                    'model': model,\n",
    "                    'train_r2': train_r2,\n",
    "                    'test_r2': test_r2,\n",
    "                    'test_mse': test_mse,\n",
    "                    'test_mae': test_mae,\n",
    "                    'test_rmse': test_rmse,\n",
    "                    'predictions': y_pred_test\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ {name}:\")\n",
    "                print(f\"   - Train R¬≤: {train_r2:.4f}\")\n",
    "                print(f\"   - Test R¬≤: {test_r2:.4f}\")\n",
    "                print(f\"   - Test RMSE: {test_rmse:.2f}\")\n",
    "                print(f\"   - Test MAE: {test_mae:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error training {name}: {str(e)}\")\n",
    "        \n",
    "        # Find best regression model\n",
    "        if regression_scores:\n",
    "            best_reg_model = max(regression_scores.items(), key=lambda x: x[1]['test_r2'])\n",
    "            print(f\"\\nüèÜ BEST REGRESSION MODEL: {best_reg_model[0]} (R¬≤ = {best_reg_model[1]['test_r2']:.4f})\")\n",
    "            modeling_results['best_regression'] = best_reg_model\n",
    "    \n",
    "    # ===== STEP 4: CLASSIFICATION MODELS =====\n",
    "    if 'classification' in modeling_results:\n",
    "        print(f\"\\n\" + \"=\" * 70)\n",
    "        print(\"STEP 4: CLASSIFICATION MODELS - PREDICTING PRODUCTIVITY CATEGORY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        class_data = modeling_results['classification']\n",
    "        \n",
    "        # Initialize classification models\n",
    "        classification_models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "            'SVM': SVC(random_state=42, probability=True)\n",
    "        }\n",
    "        \n",
    "        classification_scores = {}\n",
    "        \n",
    "        print(\"üîß Training classification models...\")\n",
    "        \n",
    "        for name, model in classification_models.items():\n",
    "            try:\n",
    "                # Train the model\n",
    "                model.fit(class_data['X_train'], class_data['y_train'])\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred_train = model.predict(class_data['X_train'])\n",
    "                y_pred_test = model.predict(class_data['X_test'])\n",
    "                \n",
    "                # Calculate metrics\n",
    "                train_acc = accuracy_score(class_data['y_train'], y_pred_train)\n",
    "                test_acc = accuracy_score(class_data['y_test'], y_pred_test)\n",
    "                test_precision = precision_score(class_data['y_test'], y_pred_test, average='weighted')\n",
    "                test_recall = recall_score(class_data['y_test'], y_pred_test, average='weighted')\n",
    "                test_f1 = f1_score(class_data['y_test'], y_pred_test, average='weighted')\n",
    "                \n",
    "                classification_scores[name] = {\n",
    "                    'model': model,\n",
    "                    'train_accuracy': train_acc,\n",
    "                    'test_accuracy': test_acc,\n",
    "                    'test_precision': test_precision,\n",
    "                    'test_recall': test_recall,\n",
    "                    'test_f1': test_f1,\n",
    "                    'predictions': y_pred_test\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ {name}:\")\n",
    "                print(f\"   - Train Accuracy: {train_acc:.4f}\")\n",
    "                print(f\"   - Test Accuracy: {test_acc:.4f}\")\n",
    "                print(f\"   - Test F1-Score: {test_f1:.4f}\")\n",
    "                print(f\"   - Test Precision: {test_precision:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error training {name}: {str(e)}\")\n",
    "        \n",
    "        # Find best classification model\n",
    "        if classification_scores:\n",
    "            best_class_model = max(classification_scores.items(), key=lambda x: x[1]['test_f1'])\n",
    "            print(f\"\\nüèÜ BEST CLASSIFICATION MODEL: {best_class_model[0]} (F1 = {best_class_model[1]['test_f1']:.4f})\")\n",
    "            modeling_results['best_classification'] = best_class_model\n",
    "    \n",
    "    # ===== STEP 5: MODEL VISUALIZATION =====\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"STEP 5: MODEL PERFORMANCE VISUALIZATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Regression Model Comparison\n",
    "    if 'regression' in modeling_results and regression_scores:\n",
    "        print(\"üìä Creating regression model comparison plots...\")\n",
    "        \n",
    "        # Create comparison plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # R¬≤ Score comparison\n",
    "        models = list(regression_scores.keys())\n",
    "        r2_scores = [regression_scores[model]['test_r2'] for model in models]\n",
    "        \n",
    "        axes[0].bar(models, r2_scores, color='skyblue', alpha=0.7)\n",
    "        axes[0].set_title('Regression Models - R¬≤ Score Comparison')\n",
    "        axes[0].set_ylabel('R¬≤ Score')\n",
    "        axes[0].tick_params(axis="
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#seven></a>\n",
    "## **Evaluation and Validation**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Evaluate and validate the effectiveness and accuracy of the models.\n",
    "* **Details:** Present metrics used to evaluate the models, such as accuracy, precision, recall, F1-score, etc. Discuss validation techniques employed, such as cross-validation or train/test split.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "#Please use code cells to code in and do not forget to comment your code."
=======
    "#Please use code cells to code in and do not forget to comment your code.\n",
    "# Model Evaluation and Validation - Indian Agriculture Dataset\n",
    "# This section evaluates and validates the effectiveness and accuracy of the trained models\n",
    "\n",
    "print(\"=== MODEL EVALUATION AND VALIDATION SECTION ===\\n\")\n",
    "\n",
    "# Import additional evaluation libraries\n",
    "from sklearn.model_selection import cross_val_score, validation_curve, learning_curve\n",
    "from sklearn.metrics import mean_absolute_percentage_error, explained_variance_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    # Check if modeling results are available\n",
    "    if 'modeling_results' not in globals():\n",
    "        print(\"‚ùå No modeling results found. Please run the modeling section first.\")\n",
    "        raise RuntimeError(\"Modeling results not available\")\n",
    "    \n",
    "    print(\"‚úÖ Using results from previous modeling section\")\n",
    "    results = modeling_results\n",
    "    \n",
    "    # ===== STEP 1: COMPREHENSIVE MODEL EVALUATION =====\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STEP 1: COMPREHENSIVE MODEL EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    \n",
    "    # ===== REGRESSION MODEL EVALUATION =====\n",
    "    if 'regression' in results and 'best_regression' in results:\n",
    "        print(\"\\nüéØ REGRESSION MODEL EVALUATION - Production Prediction\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        reg_data = results['regression']\n",
    "        best_reg_name, best_reg_results = results['best_regression']\n",
    "        \n",
    "        # Get all regression models for comparison\n",
    "        regression_models = {}\n",
    "        if 'regression' in results:\n",
    "            # Re-train models to get access to model objects\n",
    "            from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "            from sklearn.tree import DecisionTreeRegressor\n",
    "            from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "            \n",
    "            models_to_test = {\n",
    "                'Linear Regression': LinearRegression(),\n",
    "                'Ridge Regression': Ridge(alpha=1.0),\n",
    "                'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10),\n",
    "                'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42, max_depth=6)\n",
    "            }\n",
    "            \n",
    "            regression_evaluation = {}\n",
    "            \n",
    "            for name, model in models_to_test.items():\n",
    "                try:\n",
    "                    # Train the model\n",
    "                    model.fit(reg_data['X_train'], reg_data['y_train'])\n",
    "                    \n",
    "                    # Make predictions\n",
    "                    y_pred_train = model.predict(reg_data['X_train'])\n",
    "                    y_pred_test = model.predict(reg_data['X_test'])\n",
    "                    \n",
    "                    # Calculate comprehensive metrics\n",
    "                    train_r2 = r2_score(reg_data['y_train'], y_pred_train)\n",
    "                    test_r2 = r2_score(reg_data['y_test'], y_pred_test)\n",
    "                    test_mse = mean_squared_error(reg_data['y_test'], y_pred_test)\n",
    "                    test_mae = mean_absolute_error(reg_data['y_test'], y_pred_test)\n",
    "                    test_rmse = np.sqrt(test_mse)\n",
    "                    test_mape = mean_absolute_percentage_error(reg_data['y_test'], y_pred_test) * 100\n",
    "                    test_evs = explained_variance_score(reg_data['y_test'], y_pred_test)\n",
    "                    \n",
    "                    # Cross-validation\n",
    "                    cv_scores = cross_val_score(model, reg_data['X_train'], reg_data['y_train'], \n",
    "                                              cv=5, scoring='r2', n_jobs=-1)\n",
    "                    \n",
    "                    regression_evaluation[name] = {\n",
    "                        'model': model,\n",
    "                        'train_r2': train_r2,\n",
    "                        'test_r2': test_r2,\n",
    "                        'test_mse': test_mse,\n",
    "                        'test_mae': test_mae,\n",
    "                        'test_rmse': test_rmse,\n",
    "                        'test_mape': test_mape,\n",
    "                        'test_evs': test_evs,\n",
    "                        'cv_mean': cv_scores.mean(),\n",
    "                        'cv_std': cv_scores.std(),\n",
    "                        'predictions': y_pred_test,\n",
    "                        'residuals': reg_data['y_test'] - y_pred_test\n",
    "                    }\n",
    "                    \n",
    "                    print(f\"‚úÖ {name}:\")\n",
    "                    print(f\"   - Test R¬≤ Score: {test_r2:.4f}\")\n",
    "                    print(f\"   - Test RMSE: {test_rmse:.2f}\")\n",
    "                    print(f\"   - Test MAE: {test_mae:.2f}\")\n",
    "                    print(f\"   - Test MAPE: {test_mape:.2f}%\")\n",
    "                    print(f\"   - CV R¬≤ (mean ¬± std): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "                    print(f\"   - Explained Variance: {test_evs:.4f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error evaluating {name}: {str(e)}\")\n",
    "            \n",
    "            evaluation_results['regression'] = regression_evaluation\n",
    "    \n",
    "    # ===== CLASSIFICATION MODEL EVALUATION =====\n",
    "    if 'classification' in results and 'best_classification' in results:\n",
    "        print(f\"\\nüéØ CLASSIFICATION MODEL EVALUATION - Productivity Prediction\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        class_data = results['classification']\n",
    "        best_class_name, best_class_results = results['best_classification']\n",
    "        \n",
    "        # Re-train classification models for evaluation\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn.svm import SVC\n",
    "        \n",
    "        class_models_to_test = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=10),\n",
    "            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
    "            'SVM': SVC(random_state=42, probability=True)\n",
    "        }\n",
    "        \n",
    "        classification_evaluation = {}\n",
    "        \n",
    "        for name, model in class_models_to_test.items():\n",
    "            try:\n",
    "                # Train the model\n",
    "                model.fit(class_data['X_train'], class_data['y_train'])\n",
    "                \n",
    "                # Make predictions\n",
    "                y_pred_train = model.predict(class_data['X_train'])\n",
    "                y_pred_test = model.predict(class_data['X_test'])\n",
    "                y_pred_proba = model.predict_proba(class_data['X_test'])[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "                \n",
    "                # Calculate comprehensive metrics\n",
    "                train_acc = accuracy_score(class_data['y_train'], y_pred_train)\n",
    "                test_acc = accuracy_score(class_data['y_test'], y_pred_test)\n",
    "                test_precision = precision_score(class_data['y_test'], y_pred_test, average='weighted')\n",
    "                test_recall = recall_score(class_data['y_test'], y_pred_test, average='weighted')\n",
    "                test_f1 = f1_score(class_data['y_test'], y_pred_test, average='weighted')\n",
    "                \n",
    "                # ROC AUC if binary classification\n",
    "                test_auc = None\n",
    "                if y_pred_proba is not None and len(np.unique(class_data['y_test'])) == 2:\n",
    "                    test_auc = roc_auc_score(class_data['y_test'], y_pred_proba)\n",
    "                \n",
    "                # Cross-validation\n",
    "                cv_scores = cross_val_score(model, class_data['X_train'], class_data['y_train'], \n",
    "                                          cv=5, scoring='f1_weighted', n_jobs=-1)\n",
    "                \n",
    "                classification_evaluation[name] = {\n",
    "                    'model': model,\n",
    "                    'train_accuracy': train_acc,\n",
    "                    'test_accuracy': test_acc,\n",
    "                    'test_precision': test_precision,\n",
    "                    'test_recall': test_recall,\n",
    "                    'test_f1': test_f1,\n",
    "                    'test_auc': test_auc,\n",
    "                    'cv_mean': cv_scores.mean(),\n",
    "                    'cv_std': cv_scores.std(),\n",
    "                    'predictions': y_pred_test,\n",
    "                    'probabilities': y_pred_proba\n",
    "                }\n",
    "                \n",
    "                print(f\"‚úÖ {name}:\")\n",
    "                print(f\"   - Test Accuracy: {test_acc:.4f}\")\n",
    "                print(f\"   - Test F1-Score: {test_f1:.4f}\")\n",
    "                print(f\"   - Test Precision: {test_precision:.4f}\")\n",
    "                print(f\"   - Test Recall: {test_recall:.4f}\")\n",
    "                if test_auc is not None:\n",
    "                    print(f\"   - Test ROC AUC: {test_auc:.4f}\")\n",
    "                print(f\"   - CV F1 (mean ¬± std): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error evaluating {name}: {str(e)}\")\n",
    "        \n",
    "        evaluation_results['classification'] = classification_evaluation\n",
    "    \n",
    "    # ===== STEP 2: DETAILED VALIDATION ANALYSIS =====\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 2: DETAILED VALIDATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ===== REGRESSION VALIDATION =====\n",
    "    if 'regression' in evaluation_results:\n",
    "        print(\"\\nüìä REGRESSION VALIDATION ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        reg_eval = evaluation_results['regression']\n",
    "        reg_data = results['regression']\n",
    "        \n",
    "        # Find best model based on cross-validation\n",
    "        best_reg_cv = max(reg_eval.items(), key=lambda x: x[1]['cv_mean'])\n",
    "        print(f\"üèÜ Best Regression Model (CV): {best_reg_cv[0]} (CV R¬≤ = {best_reg_cv[1]['cv_mean']:.4f})\")\n",
    "        \n",
    "        # Residual Analysis for best model\n",
    "        best_model_name, best_model_results = best_reg_cv\n",
    "        residuals = best_model_results['residuals']\n",
    "        predictions = best_model_results['predictions']\n",
    "        \n",
    "        # Create residual plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Residuals vs Predicted\n",
    "        axes[0, 0].scatter(predictions, residuals, alpha=0.6)\n",
    "        axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
    "        axes[0, 0].set_xlabel('Predicted Values')\n",
    "        axes[0, 0].set_ylabel('Residuals')\n",
    "        axes[0, 0].set_title('Residuals vs Predicted')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. QQ Plot of residuals\n",
    "        from scipy import stats\n",
    "        stats.probplot(residuals, dist=\"norm\", plot=axes[0, 1])\n",
    "        axes[0, 1].set_title('Q-Q Plot of Residuals')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Histogram of residuals\n",
    "        axes[1, 0].hist(residuals, bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 0].set_xlabel('Residuals')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].set_title('Distribution of Residuals')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Actual vs Predicted\n",
    "        axes[1, 1].scatter(reg_data['y_test'], predictions, alpha=0.6)\n",
    "        min_val = min(reg_data['y_test'].min(), predictions.min())\n",
    "        max_val = max(reg_data['y_test'].max(), predictions.max())\n",
    "        axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        axes[1, 1].set_xlabel('Actual Values')\n",
    "        axes[1, 1].set_ylabel('Predicted Values')\n",
    "        axes[1, 1].set_title('Actual vs Predicted')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'Regression Validation Analysis - {best_model_name}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Statistical tests on residuals\n",
    "        print(f\"\\nüìà RESIDUAL ANALYSIS - {best_model_name}:\")\n",
    "        \n",
    "        # Normality test (Shapiro-Wilk)\n",
    "        if len(residuals) <= 5000:  # Shapiro-Wilk works best for smaller samples\n",
    "            shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
    "            print(f\"   - Shapiro-Wilk Normality Test: statistic={shapiro_stat:.4f}, p-value={shapiro_p:.4f}\")\n",
    "            if shapiro_p > 0.05:\n",
    "                print(\"     ‚úÖ Residuals appear normally distributed\")\n",
    "            else:\n",
    "                print(\"     ‚ö†Ô∏è  Residuals may not be normally distributed\")\n",
    "        \n",
    "        # Homoscedasticity (constant variance)\n",
    "        residual_std = np.std(residuals)\n",
    "        print(f\"   - Residual Standard Deviation: {residual_std:.4f}\")\n",
    "        print(f\"   - Mean Absolute Residual: {np.mean(np.abs(residuals)):.4f}\")\n",
    "    \n",
    "    # ===== CLASSIFICATION VALIDATION =====\n",
    "    if 'classification' in evaluation_results:\n",
    "        print(f\"\\nüìä CLASSIFICATION VALIDATION ANALYSIS\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        class_eval = evaluation_results['classification']\n",
    "        class_data = results['classification']\n",
    "        \n",
    "        # Find best model based on cross-validation\n",
    "        best_class_cv = max(class_eval.items(), key=lambda x: x[1]['cv_mean'])\n",
    "        print(f\"üèÜ Best Classification Model (CV): {best_class_cv[0]} (CV F1 = {best_class_cv[1]['cv_mean']:.4f})\")\n",
    "        \n",
    "        best_class_name, best_class_results = best_class_cv\n",
    "        \n",
    "        # Create classification validation plots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Confusion Matrix\n",
    "        cm = confusion_matrix(class_data['y_test'], best_class_results['predictions'])\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "                   xticklabels=['Low Productivity', 'High Productivity'],\n",
    "                   yticklabels=['Low Productivity', 'High Productivity'])\n",
    "        axes[0, 0].set_title('Confusion Matrix')\n",
    "        axes[0, 0].set_ylabel('Actual')\n",
    "        axes[0, 0].set_xlabel('Predicted')\n",
    "        \n",
    "        # 2. ROC Curve (if probabilities available)\n",
    "        if best_class_results['probabilities'] is not None and best_class_results['test_auc'] is not None:\n",
    "            fpr, tpr, _ = roc_curve(class_data['y_test'], best_class_results['probabilities'])\n",
    "            axes[0, 1].plot(fpr, tpr, linewidth=2, \n",
    "                           label=f'ROC Curve (AUC = {best_class_results[\"test_auc\"]:.3f})')\n",
    "            axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "            axes[0, 1].set_xlabel('False Positive Rate')\n",
    "            axes[0, 1].set_ylabel('True Positive Rate')\n",
    "            axes[0, 1].set_title('ROC Curve')\n",
    "            axes[0, 1].legend()\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, 'ROC Curve\\nNot Available', \n",
    "                           ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title('ROC Curve')\n",
    "        \n",
    "        # 3. Precision-Recall Curve\n",
    "        if best_class_results['probabilities'] is not None:\n",
    "            precision, recall, _ = precision_recall_curve(class_data['y_test'], \n",
    "                                                         best_class_results['probabilities'])\n",
    "            axes[1, 0].plot(recall, precision, linewidth=2)\n",
    "            axes[1, 0].set_xlabel('Recall')\n",
    "            axes[1, 0].set_ylabel('Precision')\n",
    "            axes[1, 0].set_title('Precision-Recall Curve')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, 'Precision-Recall\\nCurve Not Available', \n",
    "                           ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title('Precision-Recall Curve')\n",
    "        \n",
    "        # 4. Feature Importance (if available)\n",
    "        if hasattr(best_class_results['model'], 'feature_importances_'):\n",
    "            feature_names = class_data['feature_names']\n",
    "            importances = best_class_results['model'].feature_importances_\n",
    "            \n",
    "            # Sort by importance\n",
    "            indices = np.argsort(importances)[::-1][:10]  # Top 10\n",
    "            \n",
    "            axes[1, 1].bar(range(len(indices)), importances[indices])\n",
    "            axes[1, 1].set_xlabel('Features')\n",
    "            axes[1, 1].set_ylabel('Importance')\n",
    "            axes[1, 1].set_title('Top 10 Feature Importances')\n",
    "            axes[1, 1].set_xticks(range(len(indices)))\n",
    "            axes[1, 1].set_xticklabels([feature_names[i] for i in indices], rotation=45)\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, 'Feature Importance\\nNot Available', \n",
    "                           ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title('Feature Importances')\n",
    "        \n",
    "        plt.suptitle(f'Classification Validation Analysis - {best_class_name}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # ===== STEP 3: CROSS-VALIDATION ANALYSIS =====\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 3: CROSS-VALIDATION ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Cross-validation comparison plot\n",
    "    if 'regression' in evaluation_results:\n",
    "        print(\"üìä Regression Models Cross-Validation Comparison:\")\n",
    "        \n",
    "        models = list(evaluation_results['regression'].keys())\n",
    "        cv_means = [evaluation_results['regression'][model]['cv_mean'] for model in models]\n",
    "        cv_stds = [evaluation_results['regression'][model]['cv_std'] for model in models]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x_pos = np.arange(len(models))\n",
    "        plt.bar(x_pos, cv_means, yerr=cv_stds, alpha=0.7, capsize=5)\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Cross-Validation R¬≤ Score')\n",
    "        plt.title('Regression Models - Cross-Validation Performance')\n",
    "        plt.xticks(x_pos, models, rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print detailed CV results\n",
    "        for model in models:\n",
    "            cv_mean = evaluation_results['regression'][model]['cv_mean']\n",
    "            cv_std = evaluation_results['regression'][model]['cv_std']\n",
    "            print(f\"   - {model}: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "    \n",
    "    if 'classification' in evaluation_results:\n",
    "        print(f\"\\nüìä Classification Models Cross-Validation Comparison:\")\n",
    "        \n",
    "        models = list(evaluation_results['classification'].keys())\n",
    "        cv_means = [evaluation_results['classification'][model]['cv_mean'] for model in models]\n",
    "        cv_stds = [evaluation_results['classification'][model]['cv_std'] for model in models]\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        x_pos = np.arange(len(models))\n",
    "        plt.bar(x_pos, cv_means, yerr=cv_stds, alpha=0.7, capsize=5)\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Cross-Validation F1 Score')\n",
    "        plt.title('Classification Models - Cross-Validation Performance')\n",
    "        plt.xticks(x_pos, models, rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        for model in models:\n",
    "            cv_mean = evaluation_results['classification'][model]['cv_mean']\n",
    "            cv_std = evaluation_results['classification'][model]['cv_std']\n",
    "            print(f\"   - {model}: {cv_mean:.4f} ¬± {cv_std:.4f}\")\n",
    "    \n",
    "    # ===== STEP 4: MODEL COMPARISON SUMMARY =====\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 4: MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if 'regression' in evaluation_results:\n",
    "        print(\"üìä REGRESSION MODEL RANKING:\")\n",
    "        reg_models = evaluation_results['regression']\n",
    "        \n",
    "        # Rank by test R¬≤\n",
    "        ranked_by_r2 = sorted(reg_models.items(), key=lambda x: x[1]['test_r2'], reverse=True)\n",
    "        print(\"   By Test R¬≤ Score:\")\n",
    "        for i, (model, results) in enumerate(ranked_by_r2, 1):\n",
    "            print(f\"   {i}. {model}: R¬≤ = {results['test_r2']:.4f}, RMSE = {results['test_rmse']:.2f}\")\n",
    "        \n",
    "        # Rank by CV score\n",
    "        ranked_by_cv = sorted(reg_models.items(), key=lambda x: x[1]['cv_mean'], reverse=True)\n",
    "        print(f\"\\n   By Cross-Validation R¬≤ Score:\")\n",
    "        for i, (model, results) in enumerate(ranked_by_cv, 1):\n",
    "            print(f\"   {i}. {model}: CV R¬≤ = {results['cv_mean']:.4f} ¬± {results['cv_std']:.4f}\")\n",
    "    \n",
    "    if 'classification' in evaluation_results:\n",
    "        print(f\"\\nüìä CLASSIFICATION MODEL RANKING:\")\n",
    "        class_models = evaluation_results['classification']\n",
    "        \n",
    "        # Rank by test F1\n",
    "        ranked_by_f1 = sorted(class_models.items(), key=lambda x: x[1]['test_f1'], reverse=True)\n",
    "        print(\"   By Test F1 Score:\")\n",
    "        for i, (model, results) in enumerate(ranked_by_f1, 1):\n",
    "            auc_text = f\", AUC = {results['test_auc']:.4f}\" if results['test_auc'] is not None else \"\"\n",
    "            print(f\"   {i}. {model}: F1 = {results['test_f1']:.4f}, Acc = {results['test_accuracy']:.4f}{auc_text}\")\n",
    "        \n",
    "        # Rank by CV score\n",
    "        ranked_by_cv = sorted(class_models.items(), key=lambda x: x[1]['cv_mean'], reverse=True)\n",
    "        print(f\"\\n   By Cross-Validation F1 Score:\")\n",
    "        for i, (model, results) in enumerate(ranked_by_cv, 1):\n",
    "            print(f\"   {i}. {model}: CV F1 = {results['cv_mean']:.4f} ¬± {results['cv_std']:.4f}\")\n",
    "    \n",
    "    # ===== STEP 5: VALIDATION CONCLUSIONS =====\n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"STEP 5: VALIDATION CONCLUSIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"üéØ MODEL VALIDATION SUMMARY:\")\n",
    "    \n",
    "    if 'regression' in evaluation_results:\n",
    "        best_reg = max(evaluation_results['regression'].items(), key=lambda x: x[1]['cv_mean'])\n",
    "        print(f\"\\n   üìà REGRESSION (Production Prediction):\")\n",
    "        print(f\"      - Recommended Model: {best_reg[0]}\")\n",
    "        print(f\"      - Cross-Validation R¬≤: {best_reg[1]['cv_mean']:.4f} ¬± {best_reg[1]['cv_std']:.4f}\")\n",
    "        print(f\"      - Test Set R¬≤: {best_reg[1]['test_r2']:.4f}\")\n",
    "        print(f\"      - Test Set RMSE: {best_reg[1]['test_rmse']:.2f}\")\n",
    "        print(f\"      - Test Set MAPE: {best_reg[1]['test_mape']:.2f}%\")\n",
    "        \n",
    "        # Model reliability assessment\n",
    "        r2_diff = abs(best_reg[1]['train_r2'] - best_reg[1]['test_r2'])\n",
    "        if r2_diff < 0.05:\n",
    "            print(f\"      ‚úÖ Model shows good generalization (train-test R¬≤ diff: {r2_diff:.4f})\")\n",
    "        elif r2_diff < 0.1:\n",
    "            print(f\"      ‚ö†Ô∏è  Model shows moderate overfitting (train-test R¬≤ diff: {r2_diff:.4f})\")\n",
    "        else:\n",
    "            print(f\"      ‚ùå Model shows significant overfitting (train-test R¬≤ diff: {r2_diff:.4f})\")\n",
    "    \n",
    "    if 'classification' in evaluation_results:\n",
    "        best_class = max(evaluation_results['classification'].items(), key=lambda x: x[1]['cv_mean'])\n",
    "        print(f\"\\n   üìä CLASSIFICATION (Productivity Prediction):\")\n",
    "        print(f\"      - Recommended Model: {best_class[0]}\")\n",
    "        print(f\"      - Cross-Validation F1: {best_class[1]['cv_mean']:.4f} ¬± {best_class[1]['cv_std']:.4f}\")\n",
    "        print(f\"      - Test Set F1: {best_class[1]['test_f1']:.4f}\")\n",
    "        print(f\"      - Test Set Accuracy: {best_class[1]['test_accuracy']:.4f}\")\n",
    "        if best_class[1]['test_auc'] is not None:\n",
    "            print(f\"      - Test Set ROC AUC: {best_class[1]['test_auc']:.4f}\")\n",
    "        \n",
    "        # Model reliability assessment\n",
    "        acc_diff = abs(best_class[1]['train_accuracy'] - best_class[1]['test_accuracy'])\n",
    "        if acc_diff < 0.05:\n",
    "            print(f\"      ‚úÖ Model shows good generalization (train-test acc diff: {acc_diff:.4f})\")\n",
    "        elif acc_diff < 0.1:\n",
    "            print(f\"      ‚ö†Ô∏è  Model shows moderate overfitting (train-test acc diff: {acc_diff:.4f})\")\n",
    "        else:\n",
    "            print(f\"      ‚ùå Model shows significant overfitting (train-test acc diff: {acc_diff:.4f})\")\n",
    "    \n",
    "    # Store evaluation results for next sections\n",
    "    globals()['evaluation_results'] = evaluation_results\n",
    "    \n",
    "    print(f\"\\n‚úÖ Evaluation results stored in 'evaluation_results' variable\")\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"MODEL EVALUATION AND VALIDATION SECTION COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR DURING EVALUATION: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nüîß TROUBLESHOOTING:\")\n",
    "    print(\"1. Ensure the modeling section was completed successfully\")\n",
    "    print(\"2. Check that 'modeling_results' variable exists\")\n",
    "    print(\"3. Verify that models were trained properly\")"
>>>>>>> d432f35b8f79d9fc165677d66ff1639d95ad1752
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#eight></a>\n",
    "## **Final Model**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Present the final model and its performance.\n",
    "* **Details:** Highlight the best-performing model and discuss its configuration, performance, and why it was chosen over others.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#nine></a>\n",
    "## **Conclusion and Future Work**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Summarize the findings and discuss future directions.\n",
    "* **Details:** Conclude with a summary of the results, insights gained, limitations of the study, and suggestions for future projects or improvements in methodology or data collection.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a href=#ten></a>\n",
    "## **References**\n",
    "<a href=#cont>Back to Table of Contents</a>\n",
    "\n",
    "* **Purpose:** Provide citations and sources of external content.\n",
    "* **Details:** List all the references and sources consulted during the project, including data sources, research papers, and documentation for tools and libraries used.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please use code cells to code in and do not forget to comment your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Sections to Consider\n",
    "\n",
    "* ### Appendix: \n",
    "For any additional code, detailed tables, or extended data visualizations that are supplementary to the main content.\n",
    "\n",
    "* ### Contributors: \n",
    "If this is a group project, list the contributors and their roles or contributions to the project.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
